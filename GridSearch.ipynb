{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e6c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import SVD\n",
    "from surprise import AlgoBase\n",
    "from surprise import Trainset\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# import sys\n",
    "import math\n",
    "import statistics\n",
    "import collections\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import NMF, truncatedSVD\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import timeit\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2407af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingDataset:\n",
    "    import numpy as np\n",
    "    from scipy import sparse\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rating_mat = None\n",
    "        # self.time_mat = None\n",
    "        self._data_file_path = ''\n",
    "        \n",
    "        # list of raw user_IDs (dataset IDs)\n",
    "        self.items = []\n",
    "        self.users = []\n",
    "        self.item_n = 0\n",
    "        self.user_n = 0\n",
    "        \n",
    "        # maps raw user_id to user_iid(or inner id)\n",
    "        self.user_to_iid = {}\n",
    "        # maps user inner id to dataset raw ID\n",
    "        self.user_to_ID = {}\n",
    "        # maps raw item_id (dataset) to item_iid(or inner id)\n",
    "        self.item_to_iid = {}\n",
    "        # maps item inner id to dataset raw ID\n",
    "        self.item_to_ID = {}\n",
    "        \n",
    "        # list of triples of (item, rating, timestamp) for each user_iid. \n",
    "        # TODO: In case there were no Timestamp in the data, pairs of (item, rating) will be kept\n",
    "        self.user_ratings = []\n",
    "        # list of pair of (user, rating) for each item_iid\n",
    "        self.item_ratings = []\n",
    "        \n",
    "        \n",
    "    def __get_line_format_indices(self, line_format):\n",
    "        # specifying the order of 'user, item, rating, timestamp' in each line \n",
    "        lf_sp = line_format.split(' ')\n",
    "        # if len(lf_sp) != 4:\n",
    "        #     raise Exception('''Bad line format!\n",
    "        #     line_format should be space-separated and it should always specified by \n",
    "        #     \"user item rating timestamp\" with any order!''')\n",
    "        user_idx = -1\n",
    "        item_idx = -1\n",
    "        rating_idx = -1\n",
    "        # timestamp_idx = -1\n",
    "        for c in range(len(lf_sp)):\n",
    "            if lf_sp[c] == 'user':\n",
    "                user_idx = c\n",
    "            elif lf_sp[c] == 'item':\n",
    "                item_idx = c\n",
    "            elif lf_sp[c] == 'rating':\n",
    "                rating_idx = c\n",
    "            # elif lf_sp[c] == 'timestamp':\n",
    "            #     timestamp_idx = c\n",
    "            else:\n",
    "                raise Exception('line_format must be exactly dictated by one of: (user/item/rating/timestamp) separated by sep!')\n",
    "        \n",
    "        # return user_idx, item_idx, rating_idx, timestamp_idx\n",
    "        return user_idx, item_idx, rating_idx\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        Read the rating data from file and parse it and then make the dataset.\n",
    "    '''\n",
    "    # def read_from_file(self, data_fn, skip_lines=0, sep=',', line_format='user item rating timestamp'):\n",
    "    def read_from_file(self, data_fn, skip_lines=0, sep=',', line_format='user item rating'):\n",
    "        \n",
    "        # user_fmt_idx, item_fmt_idx, rating_fmt_idx, timestamp_fmt_idx = self.__get_line_format_indices(line_format)\n",
    "        user_fmt_idx, item_fmt_idx, rating_fmt_idx = self.__get_line_format_indices(line_format)\n",
    "        \n",
    "        file = open(data_fn, 'r')\n",
    "        \n",
    "        # skip lines that are specified from input\n",
    "        for _ in range(skip_lines):\n",
    "            file.readline()\n",
    "            \n",
    "        # users list as in input file\n",
    "        users_lin = []\n",
    "        items_lin = []\n",
    "        ratings_lin = []\n",
    "        # timestamps_lin = []\n",
    "\n",
    "        self.raw_ratings = []\n",
    "        for l in file:\n",
    "            lsp = l.split(sep)\n",
    "            user_id = lsp[user_fmt_idx]\n",
    "            item_id = lsp[item_fmt_idx]\n",
    "            rating = float(lsp[rating_fmt_idx])\n",
    "            # timestamp = int(lsp[timestamp_fmt_idx].strip('\\n'))\n",
    "            \n",
    "            users_lin.append(user_id)\n",
    "            items_lin.append(item_id)\n",
    "            ratings_lin.append(rating)\n",
    "            # timestamps_lin.append(timestamp)\n",
    "\n",
    "            self.raw_ratings.append((user_id, item_id, rating))\n",
    "            \n",
    "        self.users = list(set(users_lin))\n",
    "        self.items = list(set(items_lin))\n",
    "        \n",
    "        self.user_n = len(self.users)\n",
    "        self.item_n = len(self.items)\n",
    "        \n",
    "        '''note that raw ids are in STRING format, and the iid in INTEGER format!'''\n",
    "        # set the mappings\n",
    "        for idx in range(self.user_n): \n",
    "            self.user_to_iid[self.users[idx]] = idx\n",
    "            \n",
    "        for idx in range(self.user_n):\n",
    "            self.user_to_ID[idx] = self.users[idx] \n",
    "            \n",
    "        for idx in range(self.item_n):\n",
    "            self.item_to_iid[self.items[idx]] = idx \n",
    "            \n",
    "        for idx in range(self.item_n):\n",
    "            self.item_to_ID[idx] = self.items[idx] \n",
    "        \n",
    "        # init rating matrix\n",
    "        self.rating_mat = sparse.lil_matrix((self.user_n, self.item_n))\n",
    "        # self.time_mat = sparse.lil_matrix((self.user_n, self.item_n))\n",
    "        for idx in range(len(users_lin)):\n",
    "            user_iid = self.user_to_iid[users_lin[idx]]\n",
    "            item_iid = self.item_to_iid[items_lin[idx]]\n",
    "            rating = ratings_lin[idx]\n",
    "            self.rating_mat[user_iid, item_iid] = rating\n",
    "            # self.time_mat[user_iid, item_iid] = timestamps_lin[idx]\n",
    "            \n",
    "            \n",
    "    def list_users_ratings(self, rating_matrix):\n",
    "        # finding the user and item ratings\n",
    "        user_ratings = []\n",
    "        for user_iid in range(self.user_n):\n",
    "            # append a list for this user\n",
    "            user_ratings.append([])\n",
    "            user_nonze = np.nonzero(rating_matrix[user_iid])\n",
    "            for item_iid in user_nonze[1]:\n",
    "                # add items and its rating into the last user added to the list\n",
    "                user_ratings[-1].append((item_iid, rating_matrix[user_iid, item_iid]))\n",
    "                if rating_matrix[user_iid, item_iid] == 0:\n",
    "                    raise Exception('Found zero rating in nonzero ratings of user with inner id %d and item iid %d!' % (user_iid, item_iid))\n",
    "        return user_ratings\n",
    "    \n",
    "            \n",
    "    def list_items_ratings(self, rating_matrix):\n",
    "        item_ratings = []\n",
    "        for item_iid in range(self.item_n):\n",
    "            # append a list for this item\n",
    "            item_ratings.append([])\n",
    "            item_nonze = np.nonzero(rating_matrix.T[item_iid])\n",
    "            for user_iid in item_nonze[1]:\n",
    "                # add users and its rating into the last item added to the list\n",
    "                item_ratings[-1].append((user_iid, rating_matrix[user_iid, item_iid]))\n",
    "                if rating_matrix[user_iid, item_iid] == 0:\n",
    "                    raise Exception('Found zero rating in nonzero ratings of user with inner id %d and item iid %d!' % (user_iid, item_iid))\n",
    "        return item_ratings\n",
    "        \n",
    "            \n",
    "    def train_test_split(self, test_percent=0.2, least_userlen_test=10):\n",
    "        if test_percent > 1:\n",
    "            raise Exception('test_percent should be between 0 and 1.')\n",
    "            \n",
    "        user_ratings = self.list_users_ratings(self.rating_mat)\n",
    "        \n",
    "        mat = sparse.lil_matrix((self.user_n, self.item_n))\n",
    "        user_tests = {}\n",
    "        n_users_in_test = 0\n",
    "        n_ratings_in_test = 0\n",
    "        n_ratings_in_train = 0\n",
    "        \n",
    "        for user_iid in range(self.user_n):\n",
    "            len_u = len(user_ratings[user_iid])\n",
    "            if len_u >= least_userlen_test:\n",
    "                n_users_in_test += 1\n",
    "                test_len = int(len_u * test_percent)\n",
    "                test_set_u = list(range(len_u))\n",
    "#                 print(test_len, len_u)\n",
    "                random.shuffle(test_set_u)\n",
    "                \n",
    "                train_set_u = test_set_u[test_len:][:]\n",
    "                test_set_u = test_set_u[:test_len][:]\n",
    "                \n",
    "#                 print(len(train_set_u))\n",
    "                \n",
    "                for ir_idx in train_set_u:\n",
    "                    # ir = the pair of (item, rating)\n",
    "                    ir = user_ratings[user_iid][ir_idx]\n",
    "                    mat[user_iid, ir[0]] = ir[1]\n",
    "                    n_ratings_in_train += 1\n",
    "                \n",
    "                user_tests[user_iid] = []\n",
    "                for ir_idx in test_set_u:\n",
    "                    # ir = the pair of (item, rating)\n",
    "                    ir = user_ratings[user_iid][ir_idx]\n",
    "                    user_tests[user_iid].append(ir)\n",
    "                    n_ratings_in_test += 1\n",
    "                    \n",
    "            else: # if no test set should be seprated from ratings of this user\n",
    "                for ir in user_ratings[user_iid]:\n",
    "                    # ir = the pair of (item, rating)\n",
    "                    mat[user_iid, ir[0]] = ir[1]\n",
    "                    n_ratings_in_train += 1\n",
    "    \n",
    "        print('\\nNumber of users with some items in testset: %d' % n_users_in_test)\n",
    "        print('Number of ratings in trainset: %d \\t Number of ratings in testset: %d\\n' % (n_ratings_in_train, n_ratings_in_test))\n",
    "        return mat, user_tests\n",
    "    \n",
    "    def construct_trainset(self, raw_trainset):\n",
    "\n",
    "        raw2inner_id_users = {}\n",
    "        raw2inner_id_items = {}\n",
    "\n",
    "        current_u_index = 0\n",
    "        current_i_index = 0\n",
    "\n",
    "        ur = defaultdict(list)\n",
    "        ir = defaultdict(list)\n",
    "\n",
    "        # user raw id, item raw id, translated rating, time stamp\n",
    "        for urid, irid, r, timestamp in raw_trainset:\n",
    "            try:\n",
    "                uid = raw2inner_id_users[urid]\n",
    "            except KeyError:\n",
    "                uid = current_u_index\n",
    "                raw2inner_id_users[urid] = current_u_index\n",
    "                current_u_index += 1\n",
    "            try:\n",
    "                iid = raw2inner_id_items[irid]\n",
    "            except KeyError:\n",
    "                iid = current_i_index\n",
    "                raw2inner_id_items[irid] = current_i_index\n",
    "                current_i_index += 1\n",
    "\n",
    "            ur[uid].append((iid, r))\n",
    "            ir[iid].append((uid, r))\n",
    "\n",
    "        n_users = len(ur)  # number of users\n",
    "        n_items = len(ir)  # number of items\n",
    "        n_ratings = len(raw_trainset)\n",
    "\n",
    "        trainset = Trainset(\n",
    "            ur,\n",
    "            ir,\n",
    "            n_users,\n",
    "            n_items,\n",
    "            n_ratings,\n",
    "            self.reader.rating_scale,\n",
    "            raw2inner_id_users,\n",
    "            raw2inner_id_items,\n",
    "        )\n",
    "\n",
    "        return trainset\n",
    "\n",
    "    def construct_testset(self, raw_testset):\n",
    "\n",
    "        return [(ruid, riid, r_ui_trans) for (ruid, riid, r_ui_trans, _) in raw_testset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de065773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ml-100k/udata.csv', \n",
    "                 sep=\";\", header=0, engine=\"python\")\n",
    "\n",
    "user = pd.read_csv('ml-100k/uuser.csv', \n",
    "                   sep=\";\", header=0, engine =\"python\")\n",
    "\n",
    "genre = pd.read_csv('ml-100k/ugenre.csv', \n",
    "                    sep=\";\", header=0, engine = \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548adcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating\n",
       "0   196   242       3\n",
       "1   186   302       3\n",
       "2    22   377       1\n",
       "3   244    51       2\n",
       "4   166   346       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eada72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_core = 45\n",
    "filter_items = df['item'].value_counts() > 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d6002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data frame shape:\t(100000, 3)\n",
      "The new data frame shape:\t(76420, 3)\n",
      "\n",
      "The original data frame shape:\t(76420, 3)\n",
      "The new data frame shape:\t(73656, 3)\n",
      "\n",
      "The original data frame shape:\t(73656, 3)\n",
      "The new data frame shape:\t(73003, 3)\n",
      "\n",
      "The original data frame shape:\t(73003, 3)\n",
      "The new data frame shape:\t(72690, 3)\n",
      "\n",
      "The original data frame shape:\t(72690, 3)\n",
      "The new data frame shape:\t(72600, 3)\n",
      "\n",
      "The original data frame shape:\t(72600, 3)\n",
      "The new data frame shape:\t(72600, 3)\n",
      "\n",
      "(72600, 3)\n",
      "   user  item  rating\n",
      "1   186   302       3\n",
      "3   244    51       2\n",
      "5   298   474       4\n",
      "6   115   265       2\n",
      "7   253   465       5\n",
      "\n",
      "#users:  (551,)\n",
      "#items:  (593,)\n"
     ]
    }
   ],
   "source": [
    "# To reduce the dimensionality of the dataset,\n",
    "# we will filter out rarely rated movies and rarely rating users\n",
    "\n",
    "min_ratings = n_core\n",
    "min_user_ratings = n_core\n",
    "\n",
    "init_df = df\n",
    "init_shp = df.shape[0]\n",
    "filt_shp = 0.0\n",
    "\n",
    "while True:\n",
    "\n",
    "    filter_items = init_df['item'].value_counts() > min_ratings\n",
    "    filter_items = filter_items[filter_items == True].index.tolist()\n",
    "\n",
    "    filter_users = init_df['user'].value_counts() > min_user_ratings\n",
    "    filter_users = filter_users[filter_users == True].index.tolist()\n",
    "\n",
    "    filt_df = init_df[(init_df['item'].isin(filter_items)) & (init_df['user'].isin(filter_users))]\n",
    "\n",
    "    print('The original data frame shape:\\t{}'.format(init_df.shape))\n",
    "    print('The new data frame shape:\\t{}'.format(filt_df.shape))\n",
    "    print()\n",
    "    \n",
    "    init_shp = init_df.shape[0]\n",
    "    filt_shp = filt_df.shape[0]\n",
    "    \n",
    "    # print(init_shp, filt_shp)\n",
    "    \n",
    "    if (init_shp == filt_shp):\n",
    "        break\n",
    "    \n",
    "    init_df = filt_df\n",
    "\n",
    "    \n",
    "#------------------------------------------------------------\n",
    "'Updating the df to its filtered version'\n",
    "'Now filt-df is called df.' \n",
    "df = filt_df\n",
    "print(filt_df.shape)\n",
    "print(df.head())\n",
    "\n",
    "print()\n",
    "print('#users: ', np.unique(df['user']).shape)\n",
    "print('#items: ', np.unique(df['item']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20299d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./filtered_ml_%icore.csv'%n_core, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c91dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users 551\n",
      "# items 593\n",
      "\n",
      "Number of users with some items in testset: 551\n",
      "Number of ratings in trainset: 58307 \t Number of ratings in testset: 14293\n",
      "\n",
      "(551, 593)\n"
     ]
    }
   ],
   "source": [
    "# data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)\n",
    "\n",
    "dataset = RatingDataset()\n",
    "data_fn = './filtered_ml_%icore.csv'%n_core\n",
    "dataset.read_from_file(data_fn, skip_lines=1, line_format='user item rating', sep=',')\n",
    "\n",
    "\n",
    "print('# users', dataset.user_n)\n",
    "print('# items', dataset.item_n)\n",
    "\n",
    "\n",
    "# user_tests is the test_mat\n",
    "train_mat, test_mat = dataset.train_test_split(test_percent=0.2, least_userlen_test=10)\n",
    "\n",
    "\n",
    "# ir = the pair of (item, rating)\n",
    "# ir = user_ratings[user_iid][ir_idx]\n",
    "user_ratings = dataset.list_users_ratings(dataset.rating_mat)\n",
    "print(dataset.rating_mat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a7a49e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating\n",
       "0     0     1     2.0\n",
       "1     0     6     3.0\n",
       "2     0     7     4.0\n",
       "3     0     9     5.0\n",
       "4     0    11     4.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = dataset.list_users_ratings(train_mat)\n",
    "# user_ratings[0]\n",
    "# train_df = pd.DataFrame(columns = ['item','rating'])\n",
    "\n",
    "\n",
    "tr_lst = []\n",
    "\n",
    "for user_iid in range(dataset.user_n): \n",
    "    # trainset or dataset.user_n?? we keep the users the same (user-fixed) so they are equal.\n",
    "    \n",
    "    if user_ratings[user_iid]:\n",
    "        base_rec = pd.DataFrame(user_ratings[user_iid])\n",
    "        base_rec[2] = user_iid\n",
    "\n",
    "        # base_rec[3] = 0\n",
    "\n",
    "        tr_lst.append(base_rec[[2,0,1]])\n",
    "        # tr_lst.append(base_rec[[2,0,1,3]])\n",
    "    \n",
    "\n",
    "train_df = pd.concat(tr_lst, ignore_index=True)\n",
    "train_df.columns = ['user','item','rating']\n",
    "# train_df.columns = ['user','item','rating','timestamp']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c09072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>515</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating\n",
       "0     0     4     4.0\n",
       "1     0   379     3.0\n",
       "2     0    93     4.0\n",
       "3     0   159     3.0\n",
       "4     0   515     2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set to a dataframe\n",
    "test_lst = []\n",
    "\n",
    "for uiid in test_mat.keys():\n",
    "    base_rec = pd.DataFrame(test_mat[uiid])\n",
    "    base_rec[2] = uiid\n",
    "\n",
    "    # base_rec[3] = 0\n",
    "\n",
    "    test_lst.append(base_rec[[2,0,1]])\n",
    "    # test_lst.append(base_rec[[2,0,1,3]])\n",
    "\n",
    "\n",
    "test_df = pd.concat(test_lst, ignore_index=True)\n",
    "test_df.columns = ['user','item','rating']\n",
    "# test_df.columns = ['user','item','rating','timestamp']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037e3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tstart = datetime.now()\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# oracle_SVD = SVD()\n",
    "\n",
    "# rmse_lst = []\n",
    "\n",
    "# build the train into surprise format\n",
    "# trainset_all = Dataset.load_from_df(train_df[['user', 'item', 'rating']], reader).build_full_trainset()\n",
    "trainset_all = Dataset.load_from_df(train_df[['user','item','rating']], reader)\n",
    "\n",
    "# build test set into surpirse format\n",
    "# testset_all = Dataset.load_from_df(test_df[['user', 'item', 'rating']], reader).build_full_trainset().build_testset()\n",
    "testset_all = Dataset.load_from_df(test_df[['user', 'item', 'rating']], reader)\n",
    "\n",
    "# fit \n",
    "# oracle_SVD.fit(trainset_all)\n",
    "\n",
    "# predictions\n",
    "# fin_preds = oracle_SVD.test(testset_all)\n",
    "\n",
    "# get the RMSE\n",
    "# fin_acc = accuracy.rmse(fin_preds, verbose=False)\n",
    "\n",
    "# print\n",
    "# print(fin_acc)\n",
    "\n",
    "# tend = datetime.now()    \n",
    "# print(\"\\n in ms : \\n\")\n",
    "# print(tend-tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf509004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# ground_truth: list of items ordered by time\n",
    "def nDCG_Time(ground_truth, _recList):\n",
    "    rec_num = len(_recList) # topK\n",
    "    # ground_truth is already sorted by time\n",
    "    idealOrder = ground_truth\n",
    "    idealDCG = 0.0\n",
    "    for j in range(min(rec_num, len(idealOrder))):\n",
    "        idealDCG += ((math.pow(2.0, len(idealOrder) - j) - 1) / math.log(2.0 + j))\n",
    "\n",
    "    recDCG = 0.0\n",
    "    for j in range(rec_num):\n",
    "        item = _recList[j]\n",
    "        if item in ground_truth:\n",
    "            rank = len(ground_truth) - ground_truth.index(item) # why ground truth?\n",
    "            recDCG += ((math.pow(2.0, rank) - 1) / math.log(1.0 + j + 1))\n",
    "\n",
    "    return (recDCG / idealDCG)\n",
    "\n",
    "\n",
    "def Recall(_test_set, _recList):\n",
    "    hit = len(set(_recList).intersection(set(_test_set)))\n",
    "    return hit / float(len(_test_set))\n",
    "\n",
    "\n",
    "def Precision(_test_set, _recList):\n",
    "    hit = len(set(_recList).intersection(set(_test_set)))\n",
    "    return hit / float(len(_recList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "339af343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recNMF_2(user_iid, _est, mat, topk):\n",
    "    \n",
    "    rated_before = np.nonzero(mat[user_iid, :])[1]\n",
    "    estimations = _est[user_iid]\n",
    "    estimations[rated_before] = 0 \n",
    "    # you don't want to recommend the items to the user that have rated before duh!\n",
    "    \n",
    "    # top_items = np.argpartition(-estimations, topk)[:topk]\n",
    "    top_items = np.argsort(-estimations)[:topk]\n",
    "    top_ratings = -np.sort(-estimations)[:topk]\n",
    "    \n",
    "    return (user_iid, top_items, top_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecd19d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 0.19 secs\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "from sklearn.decomposition import NMF\n",
    "feature_n = 40\n",
    "\n",
    "mf = NMF(n_components=feature_n, init='random', random_state=2, tol=0.01,\n",
    "         solver='cd', max_iter=1000, alpha=1, beta_loss='frobenius',\n",
    "         l1_ratio=0)\n",
    "\n",
    "\n",
    "user_f = mf.fit_transform(train_mat)\n",
    "H = mf.components_\n",
    "item_f = mf.components_.T\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Process Time: %.2f secs' % (stop - start))\n",
    "start = timeit.default_timer()\n",
    "est = np.dot(user_f, item_f.T)\n",
    "res = []\n",
    "\n",
    "# Choose it to be 1000 instead of 10, and then the re-ranker will chose the final top 10\n",
    "for u in range(dataset.user_n):\n",
    "    res.append(recNMF_2(u, est, train_mat, 200))\n",
    "    \n",
    "user_recs_allinclude = {}\n",
    "for x in res:\n",
    "    user_recs_allinclude[x[0]] = x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "886c330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_rec_list = []\n",
    "for i in range(len(res)):\n",
    "    base_rec = pd.DataFrame(res[i][1:]).T\n",
    "    base_rec[2] = res[i][0]\n",
    "    u_rec_list.append(base_rec[[2,0,1]])\n",
    "\n",
    "u_rec_df = pd.concat(u_rec_list, ignore_index=True)\n",
    "\n",
    "# u_rec_df.to_csv('./ml_results/nmf_base_rec_ML_.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25c6a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 3.33 secs\n",
      " avg-precision 0.321\n",
      " avg-recall 0.243\n",
      " avg-nDCG 0.176\n"
     ]
    }
   ],
   "source": [
    "stop = timeit.default_timer()\n",
    "print('Process Time: %.2f secs' % (stop - start))\n",
    "\n",
    "p = []\n",
    "r = []\n",
    "n = []\n",
    "\n",
    "for u in test_mat.keys():\n",
    "    if len(test_mat[u]) > 0:\n",
    "        \n",
    "        test_items = [t[0] for t in test_mat[u] if t[1] >= 4]\n",
    "        \n",
    "        if len(test_items) > 0:\n",
    "            # to be comparable with the other algorithms, the list size should be the same that is 10 here.\n",
    "            top_items = user_recs_allinclude[u][:10] \n",
    "            \n",
    "            recall = Recall(test_items, top_items)\n",
    "            precision = Precision(test_items, top_items)\n",
    "            ndcg = nDCG_Time(test_items, top_items)\n",
    "\n",
    "            p.append(precision)\n",
    "            r.append(recall)\n",
    "            n.append(ndcg)\n",
    "\n",
    "print (\" avg-precision %.3f\\n avg-recall %.3f\\n avg-nDCG %.3f\" %\n",
    "       (np.average(p),np.average(r),np.average(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c5a30",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "### Active Learner Class (Surprise version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb243ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearner(AlgoBase):\n",
    "\n",
    "    def __init__(self, feature_n=40, random_state=0, max_iter=500,\n",
    "                 strategy='MaxRating', initial_n=5, epochs=10, query_n=10):\n",
    "        '''\n",
    "        Prediction-based active learner class.\n",
    "        '''\n",
    "\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "        self.feature_n = feature_n\n",
    "        self.random_state = random_state\n",
    "        self.max_iter = max_iter\n",
    "        self.decomposer = NMF(n_components=self.feature_n, init='random', random_state=self.random_state, max_iter=self.max_iter, verbose=False)\n",
    "        # self.decomposer = truncatedSVD(n_components=self.feature_n, algorithm='randomized', random_state=self.random_state, n_iter=self.max_iter)\n",
    "        \n",
    "        self.strategy = strategy\n",
    "        self.initial_n = initial_n\n",
    "        self.epochs = epochs\n",
    "        self.query_n = query_n\n",
    "\n",
    "        self.user_f = None\n",
    "        self.item_f = None \n",
    "        self.est = None\n",
    "        self.known_mat = None\n",
    "\n",
    "        # remember unavailable queries (kind of cheating?)\n",
    "        self.unavl = {i: [] for i in range(dataset.user_n)}\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        # initial fit\n",
    "        candidate_user_ratings = trainset.ur\n",
    "        initial_user_ratings = {}\n",
    "\n",
    "        for user_iid in range(dataset.user_n):\n",
    "\n",
    "            initial_user_ratings[user_iid] = []\n",
    "            len_u = len(candidate_user_ratings[user_iid])\n",
    "\n",
    "            if len_u >= self.initial_n:\n",
    "                selected_u = list(range(len_u))\n",
    "                random.shuffle(selected_u)\n",
    "                selected_u = selected_u[:self.initial_n][:]\n",
    "\n",
    "                for ir_idx in selected_u:\n",
    "                    ir = candidate_user_ratings[user_iid][ir_idx]\n",
    "                    initial_user_ratings[user_iid].append(ir)\n",
    "\n",
    "            else:\n",
    "                selected_u = list(range(len_u))\n",
    "                random.shuffle(selected_u)\n",
    "\n",
    "                for ir_idx in selected_u:\n",
    "                    ir = candidate_user_ratings[user_iid][ir_idx]\n",
    "                    initial_user_ratings[user_iid].append(ir)\n",
    "        \n",
    "        initial_train_df = self._rating_dic_to_df(initial_user_ratings)\n",
    "\n",
    "        self.known_mat = self._convert_df_to_mat(initial_train_df)\n",
    "        self.user_f = self.decomposer.fit_transform(self.known_mat)\n",
    "        self.item_f = self.decomposer.components_.T\n",
    "        self.est = np.dot(self.user_f, self.item_f.T)\n",
    "\n",
    "        # active learning process\n",
    "        for epoch in range(self.epochs):\n",
    "            for user_iid in range(dataset.user_n):\n",
    "\n",
    "                # get the items to be queried\n",
    "                query_item_lst = self.query(user_iid, self.query_n)[0]\n",
    "                candidate_df_u = pd.DataFrame(candidate_user_ratings[user_iid])\n",
    "                query_df = candidate_df_u.loc[candidate_df_u[0].isin(query_item_lst)].copy()\n",
    "                query_df[2] = user_iid\n",
    "                query_df = query_df[[2, 0, 1]]\n",
    "                query_df.columns = [0, 1, 2]\n",
    "                # add to known rating matrix\n",
    "                self.add_query(query_df)\n",
    "\n",
    "                # remember the items that are unavailable\n",
    "                unavailable_item_lst = [i for i in list(query_item_lst) if i not in list(query_df[1])]\n",
    "                self.unavl[user_iid] += unavailable_item_lst\n",
    "            \n",
    "            self.train()\n",
    "    \n",
    "    def _rating_dic_to_df(self, rating_dic):\n",
    "        lst = []\n",
    "\n",
    "        for user_iid in range(dataset.user_n):\n",
    "\n",
    "            if rating_dic[user_iid]:\n",
    "                res = pd.DataFrame(rating_dic[user_iid])\n",
    "                res[2] = user_iid\n",
    "\n",
    "                lst.append(res[[2,0,1]])\n",
    "        \n",
    "        df = pd.concat(lst, ignore_index=True)\n",
    "        df.columns = ['user', 'item', 'rating']\n",
    "        return df\n",
    "    \n",
    "    def _convert_df_to_mat(self, df):\n",
    "        '''\n",
    "        Convert DaraFrame to sparse matrix\n",
    "        \n",
    "        Arg:\n",
    "            df: DataFrame, training DataFrame\n",
    "        \n",
    "        Return:\n",
    "            mat: lil_matrix, sparse matrix containing training data\n",
    "        '''\n",
    "\n",
    "        mat = sparse.lil_matrix((dataset.user_n, dataset.item_n))\n",
    "        for _, row in df.iterrows():\n",
    "            user_iid = int(row[0])\n",
    "            item_iid = int(row[1])\n",
    "            rating = row[2]\n",
    "            mat[user_iid, item_iid] = rating\n",
    "        \n",
    "        return mat \n",
    "\n",
    "    def add_query(self, query_df):\n",
    "        '''\n",
    "        Add queried data to known matrix.\n",
    "        \n",
    "        Arg:\n",
    "            query_df: DataFrame, new data to be added.\n",
    "        '''\n",
    "\n",
    "        for _, row in query_df.iterrows():\n",
    "            user_iid = int(row[0])\n",
    "            item_iid = int(row[1])\n",
    "            rating = row[2]\n",
    "            self.known_mat[user_iid, item_iid] = rating\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Fit self.decomposer to known data.\n",
    "        '''\n",
    "\n",
    "        self.user_f = self.decomposer.fit_transform(self.known_mat)\n",
    "        self.item_f = self.decomposer.components_.T\n",
    "\n",
    "        self.est = np.dot(self.user_f, self.item_f.T)\n",
    "\n",
    "        # return self to enable learner.train().predict()\n",
    "        # return self\n",
    "    \n",
    "    def estimate(self, user_iid, item_iid):\n",
    "        '''\n",
    "        Estimate rating of user and item pair.\n",
    "\n",
    "        Args:\n",
    "            user_iid: int, inner id of the user to be predicted\n",
    "            item_iid: int, inner id of the item to be predicted\n",
    "        \n",
    "        Return:\n",
    "            float\n",
    "        '''\n",
    "\n",
    "        return self.est[user_iid, item_iid]\n",
    "    \n",
    "    def query(self, user_iid, topk):\n",
    "        '''\n",
    "        Find the indices of candidates which should be queried for true ratings.\n",
    "\n",
    "        Args:\n",
    "            user_iid: int, inner id of the user to be queried\n",
    "            topk:     int, number of queried candidates\n",
    "        \n",
    "        Return:\n",
    "            (top_items, top_ratings): (numpy int array, numpy float array)\n",
    "        '''\n",
    "        \n",
    "        rated_before = np.nonzero(self.known_mat[user_iid, :])[1]\n",
    "        unavailable = self.unavl[user_iid]\n",
    "        \n",
    "        estimations = self.est[user_iid]\n",
    "        estimations[rated_before] = 0\n",
    "        estimations[unavailable] = 0\n",
    "\n",
    "        if self.strategy == 'MaxRating':\n",
    "            # query the top 10 highest predicted ratings for each user\n",
    "            \n",
    "            top_items = np.argsort(-estimations)[:topk]    # argsort sorts in increasing order\n",
    "            top_ratings = -np.sort(-estimations)[:topk] # revert to original ratings\n",
    "\n",
    "        elif self.strategy == 'MinRating':\n",
    "            top_items = np.argsort(estimations)[:topk]\n",
    "            top_ratings = np.sort(estimations)[:topk]\n",
    "        \n",
    "        else:\n",
    "            k = int(topk/2)\n",
    "            low_items = np.argsort(estimations)[:topk-k]\n",
    "            low_ratings = np.sort(estimations)[:topk-k]\n",
    "            \n",
    "            high_items = np.argsort(-estimations)[:k]\n",
    "            high_ratings = -np.sort(-estimations)[:k]\n",
    "\n",
    "            top_items = np.concatenate((low_items, high_items))\n",
    "            top_ratings = np.concatenate((low_ratings, high_ratings))\n",
    "        \n",
    "        return (top_items, top_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6056274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'feature_n': [40,50],\n",
    "    'random_state': [0],\n",
    "    'max_iter': [10,100],\n",
    "    'strategy': ['MaxRating'],\n",
    "    'initial_n': [5],\n",
    "    'epochs': [10],\n",
    "    'query_n': [10]}\n",
    "\n",
    "gs = GridSearchCV(ActiveLearner, param_grid, measures=['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "553dc582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Anaconda3\\envs\\active-learning\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 100 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gs.fit(trainset_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27e72c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.769972996263446\n",
      "{'feature_n': 40, 'random_state': 0, 'max_iter': 100, 'strategy': 'MaxRating', 'initial_n': 5, 'epochs': 10, 'query_n': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score[\"rmse\"])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5d87f",
   "metadata": {},
   "source": [
    "## Binary Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f53b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryPredicitonLearner(AlgoBase):\n",
    "\n",
    "    def __init__(self, feature_n=40, random_state=0, max_iter=500,\n",
    "                 initial_n=5, epochs=10, query_n=10):\n",
    "        '''\n",
    "        Binary Prediction-based active learner class.\n",
    "        '''\n",
    "\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "        self.feature_n = feature_n\n",
    "        self.random_state = random_state\n",
    "        self.max_iter = max_iter\n",
    "        self.decomposer = NMF(n_components=self.feature_n, init='random', random_state=self.random_state, max_iter=self.max_iter, verbose=False)\n",
    "        # self.decomposer = truncatedSVD(n_components=self.feature_n, algorithm='randomized', random_state=self.random_state, n_iter=self.max_iter)\n",
    "        \n",
    "        self.initial_n = initial_n\n",
    "        self.epochs = epochs\n",
    "        self.query_n = query_n\n",
    "\n",
    "        self.user_f = None\n",
    "        self.item_f = None \n",
    "        self.est = None\n",
    "        self.known_mat = None\n",
    "\n",
    "        # remember unavailable queries (kind of cheating?)\n",
    "        self.unavl = {i: [] for i in range(dataset.user_n)}\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        # initial fit\n",
    "        candidate_user_ratings = trainset.ur\n",
    "        initial_user_ratings = {}\n",
    "\n",
    "        for user_iid in range(dataset.user_n):\n",
    "\n",
    "            initial_user_ratings[user_iid] = []\n",
    "            len_u = len(candidate_user_ratings[user_iid])\n",
    "\n",
    "            if len_u >= self.initial_n:\n",
    "                selected_u = list(range(len_u))\n",
    "                random.shuffle(selected_u)\n",
    "                selected_u = selected_u[:self.initial_n][:]\n",
    "\n",
    "                for ir_idx in selected_u:\n",
    "                    ir = candidate_user_ratings[user_iid][ir_idx]\n",
    "                    initial_user_ratings[user_iid].append(ir)\n",
    "\n",
    "            else:\n",
    "                selected_u = list(range(len_u))\n",
    "                random.shuffle(selected_u)\n",
    "\n",
    "                for ir_idx in selected_u:\n",
    "                    ir = candidate_user_ratings[user_iid][ir_idx]\n",
    "                    initial_user_ratings[user_iid].append(ir)\n",
    "        \n",
    "        initial_train_df = self._rating_dic_to_df(initial_user_ratings)\n",
    "\n",
    "        self.known_mat = self._convert_df_to_mat(initial_train_df)\n",
    "        self.user_f = self.decomposer.fit_transform(self.known_mat)\n",
    "        self.item_f = self.decomposer.components_.T\n",
    "        self.est = np.dot(self.user_f, self.item_f.T)\n",
    "\n",
    "        # active learning process\n",
    "        for epoch in range(self.epochs):\n",
    "            for user_iid in range(dataset.user_n):\n",
    "\n",
    "                # get the items to be queried\n",
    "                query_item_lst = self.query(user_iid, self.query_n)[0]\n",
    "                candidate_df_u = pd.DataFrame(candidate_user_ratings[user_iid])\n",
    "                query_df = candidate_df_u.loc[candidate_df_u[0].isin(query_item_lst)].copy()\n",
    "                query_df[2] = user_iid\n",
    "                query_df = query_df[[2, 0, 1]]\n",
    "                query_df.columns = [0, 1, 2]\n",
    "                # add to known rating matrix\n",
    "                self.add_query(query_df)\n",
    "\n",
    "                # remember the items that are unavailable\n",
    "                unavailable_item_lst = [i for i in list(query_item_lst) if i not in list(query_df[1])]\n",
    "                self.unavl[user_iid] += unavailable_item_lst\n",
    "            \n",
    "            self.train()\n",
    "    \n",
    "    def _rating_dic_to_df(self, rating_dic):\n",
    "        lst = []\n",
    "\n",
    "        for user_iid in range(dataset.user_n):\n",
    "\n",
    "            if rating_dic[user_iid]:\n",
    "                res = pd.DataFrame(rating_dic[user_iid])\n",
    "                res[2] = user_iid\n",
    "\n",
    "                lst.append(res[[2,0,1]])\n",
    "        \n",
    "        df = pd.concat(lst, ignore_index=True)\n",
    "        df.columns = ['user', 'item', 'rating']\n",
    "        return df\n",
    "    \n",
    "    def _convert_df_to_mat(self, df):\n",
    "        '''\n",
    "        Convert DaraFrame to sparse matrix\n",
    "        \n",
    "        Arg:\n",
    "            df: DataFrame, training DataFrame\n",
    "        \n",
    "        Return:\n",
    "            mat: lil_matrix, sparse matrix containing training data\n",
    "        '''\n",
    "\n",
    "        mat = sparse.lil_matrix((dataset.user_n, dataset.item_n))\n",
    "        for _, row in df.iterrows():\n",
    "            user_iid = int(row[0])\n",
    "            item_iid = int(row[1])\n",
    "            rating = row[2]\n",
    "            mat[user_iid, item_iid] = rating\n",
    "        \n",
    "        return mat \n",
    "\n",
    "    def add_query(self, query_df):\n",
    "        '''\n",
    "        Add queried data to known matrix.\n",
    "        \n",
    "        Arg:\n",
    "            query_df: DataFrame, new data to be added.\n",
    "        '''\n",
    "\n",
    "        for _, row in query_df.iterrows():\n",
    "            user_iid = int(row[0])\n",
    "            item_iid = int(row[1])\n",
    "            rating = row[2]\n",
    "            self.known_mat[user_iid, item_iid] = rating\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Fit self.decomposer to known data.\n",
    "        '''\n",
    "\n",
    "        self.user_f = self.decomposer.fit_transform(self.known_mat)\n",
    "        self.item_f = self.decomposer.components_.T\n",
    "\n",
    "        self.est = np.dot(self.user_f, self.item_f.T)\n",
    "\n",
    "        # return self to enable learner.train().predict()\n",
    "        # return self\n",
    "    \n",
    "    def estimate(self, user_iid, item_iid):\n",
    "        '''\n",
    "        Estimate rating of user and item pair.\n",
    "\n",
    "        Args:\n",
    "            user_iid: int, inner id of the user to be predicted\n",
    "            item_iid: int, inner id of the item to be predicted\n",
    "        \n",
    "        Return:\n",
    "            float\n",
    "        '''\n",
    "\n",
    "        return self.est[user_iid, item_iid]\n",
    "    \n",
    "    def query(self, user_iid, topk):\n",
    "        '''\n",
    "        Find the indices of candidates which should be queried for true ratings.\n",
    "\n",
    "        Args:\n",
    "            user_iid: int, inner id of the user to be queried\n",
    "            topk:     int, number of queried candidates\n",
    "        \n",
    "        Return:\n",
    "            (top_items, top_ratings): (numpy int array, numpy float array)\n",
    "        '''\n",
    "        \n",
    "        rated_before = np.nonzero(self.known_mat[user_iid, :])[1]\n",
    "        unavailable = self.unavl[user_iid]\n",
    "        \n",
    "        estimations = self.est[user_iid]\n",
    "        estimations[rated_before] = 0\n",
    "        estimations[unavailable] = 0\n",
    "\n",
    "        top_items = np.argsort(-estimations)[:topk]    # argsort sorts in increasing order\n",
    "        top_ratings = -np.sort(-estimations)[:topk] # revert to original ratings\n",
    "        \n",
    "        return (top_items, top_ratings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f96e607b91a2e2d3251db7ddf844b8ed38c6c958a32172526e41c91f117aef26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
